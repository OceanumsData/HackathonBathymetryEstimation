{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from torchsummary import summary\n",
    "import os\n",
    "import time\n",
    "\n",
    "from dataset import HackathonDataset\n",
    "from convnet import ConvNet\n",
    "from resnet import ResNet\n",
    "\n",
    "from config import DATA_DIR, DEVICE, USE_RAW, AUTO_ROTATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Ensemble:\n",
    "    \n",
    "    def __init__(self, Model, device, n_estimators):\n",
    "        self.Model = Model\n",
    "        self.instances = [self.Model(device) for i in range(n_estimators)]\n",
    "        self.performances = []\n",
    "    \n",
    "    def fit(self, train_dataloader, test_dataloader, n_epochs, print_frequency):\n",
    "        for it, instance in enumerate(self.instances):\n",
    "            print(f\"\\n=== Training instance {it+1}/{len(self.instances)} ===\\n\")\n",
    "            score = instance.fit(train_dataloader, test_dataloader, n_epochs, print_frequency)\n",
    "            self.performances.append(score)\n",
    "    \n",
    "    def predict(self, dataloader):\n",
    "        sorted_instances = [instance for _,instance in sorted(zip(self.performances,self.instances))]\n",
    "        predictions = [instance.predict(dataloader) for instance in sorted_instances[0:int(len(sorted_instances)*0.8)]]\n",
    "        return np.mean(predictions, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 4\n",
    "n_estimators = 10\n",
    "print_frequency = 3\n",
    "batch_size = 8  # High batch size often happen to not converge... So we use small batches, even if slower\n",
    "pred_batch_size = 128  # There is no problem of convergence for training batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#========================NOTE============================\n",
    "# We often have to reset the model, because it won't converge. I don't know why, but it is useful to know\n",
    "# If the training loss is stuck around 22 and the validation loss is stuck around 10,\n",
    "# reset the model by running this cell again, and relaunch training\n",
    "#========================END OF NOTE=====================\n",
    "\n",
    "dataset = HackathonDataset(DATA_DIR + 'mixed_train.csv', DATA_DIR, USE_RAW, transform=True, auto_rotate=AUTO_ROTATE)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=os.cpu_count() - 2)\n",
    "val_dataset = HackathonDataset(DATA_DIR + 'mixed_validation.csv', DATA_DIR, USE_RAW, auto_rotate=AUTO_ROTATE)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=pred_batch_size, shuffle=False, num_workers=os.cpu_count() - 2)\n",
    "model = Ensemble(ConvNet, DEVICE, n_estimators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Training instance 1/10 ===\n",
      "\n",
      "Epoch 1/4\n",
      "Number of batches viewed : 2373\n",
      "Current training loss : 8.573170372669173\n",
      "Current validation loss : 6.6105760082485165\n",
      "Number of batches viewed : 4747\n",
      "Current training loss : 7.51407811997212\n",
      "Current validation loss : 5.36526443075946\n",
      "Number of batches viewed : 7121\n",
      "Current training loss : 7.041330185273965\n",
      "Current validation loss : 4.563555572915265\n",
      "The epoch took  36.25 seconds\n",
      "Epoch 2/4\n",
      "Number of batches viewed : 2373\n",
      "Current training loss : 6.847223588219187\n",
      "Current validation loss : 5.02344570948383\n",
      "Number of batches viewed : 4747\n",
      "Current training loss : 6.756312635935147\n",
      "Current validation loss : 4.298437840356602\n",
      "Number of batches viewed : 7121\n",
      "Current training loss : 6.550409719636687\n",
      "Current validation loss : 6.6702773308190775\n",
      "The epoch took  36.52 seconds\n",
      "Epoch 3/4\n",
      "Number of batches viewed : 2373\n",
      "Current training loss : 6.475281699776348\n",
      "Current validation loss : 4.516090115224283\n",
      "Number of batches viewed : 4747\n",
      "Current training loss : 6.457915266472046\n",
      "Current validation loss : 4.1923693587460855\n",
      "Number of batches viewed : 7121\n",
      "Current training loss : 6.397157360147606\n",
      "Current validation loss : 5.408225829207052\n",
      "The epoch took  36.53 seconds\n",
      "Epoch 4/4\n",
      "Number of batches viewed : 2373\n",
      "Current training loss : 6.38572313605785\n",
      "Current validation loss : 4.661340051748621\n",
      "Number of batches viewed : 4747\n",
      "Current training loss : 6.2652975722030115\n",
      "Current validation loss : 5.567538652833052\n",
      "Number of batches viewed : 7121\n",
      "Current training loss : 6.334903846594498\n",
      "Current validation loss : 5.377186802428539\n",
      "The epoch took  37.59 seconds\n",
      "\n",
      "=== Training instance 2/10 ===\n",
      "\n",
      "Epoch 1/4\n",
      "Number of batches viewed : 2373\n",
      "Current training loss : 8.696757300413463\n",
      "Current validation loss : 8.788371191250057\n",
      "Number of batches viewed : 4747\n",
      "Current training loss : 7.471456820209586\n",
      "Current validation loss : 6.101212647956188\n",
      "Number of batches viewed : 7121\n",
      "Current training loss : 7.066500292822606\n",
      "Current validation loss : 5.9967806864911175\n",
      "The epoch took  39.72 seconds\n",
      "Epoch 2/4\n",
      "Number of batches viewed : 2373\n",
      "Current training loss : 6.841152980333632\n",
      "Current validation loss : 5.384816777987743\n",
      "Number of batches viewed : 4747\n",
      "Current training loss : 6.699364934935726\n",
      "Current validation loss : 7.0040895920100175\n",
      "Number of batches viewed : 7121\n",
      "Current training loss : 6.65776369409995\n",
      "Current validation loss : 4.604074155251811\n",
      "The epoch took  36.70 seconds\n",
      "Epoch 3/4\n",
      "Number of batches viewed : 2373\n",
      "Current training loss : 6.484107880604397\n",
      "Current validation loss : 4.826814958429712\n",
      "Number of batches viewed : 4747\n",
      "Current training loss : 6.480198380368642\n",
      "Current validation loss : 5.692209781624201\n",
      "Number of batches viewed : 7121\n",
      "Current training loss : 6.440268282390183\n",
      "Current validation loss : 4.553740415047473\n",
      "The epoch took  37.03 seconds\n",
      "Epoch 4/4\n",
      "Number of batches viewed : 2373\n",
      "Current training loss : 6.302522016606013\n",
      "Current validation loss : 3.5368446336956474\n",
      "Number of batches viewed : 4747\n",
      "Current training loss : 6.354938416593499\n",
      "Current validation loss : 5.182335998129657\n",
      "Number of batches viewed : 7121\n",
      "Current training loss : 6.282357801935104\n",
      "Current validation loss : 4.52260046962678\n",
      "The epoch took  36.26 seconds\n",
      "\n",
      "=== Training instance 3/10 ===\n",
      "\n",
      "Epoch 1/4\n",
      "Number of batches viewed : 2373\n",
      "Current training loss : 9.066702990103913\n",
      "Current validation loss : 5.821988458708515\n",
      "Number of batches viewed : 4747\n",
      "Current training loss : 7.336139455751501\n"
     ]
    }
   ],
   "source": [
    "model.fit(dataloader, val_dataloader, n_epochs, print_frequency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation on Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = HackathonDataset(DATA_DIR + 'mixed_test.csv', DATA_DIR, USE_RAW, auto_rotate=AUTO_ROTATE)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=pred_batch_size, shuffle=False, num_workers=os.cpu_count() - 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_file_names = []\n",
    "for val in test_dataloader:\n",
    "    image_file_names += val['image_file_name']\n",
    "\n",
    "predictions = model.predict(test_dataloader)\n",
    "kaggle_df = pd.DataFrame({'image_id': image_file_names,\n",
    "                          'predicted_z': predictions})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "kaggle_df.to_csv('predictions/prediction-' + datetime.now().strftime(\"%d-%m-%y:%H-%M\") + '.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
