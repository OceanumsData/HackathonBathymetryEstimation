{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from torchsummary import summary\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = 'data/'\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "USE_RAW = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_transformation(im):\n",
    "    \"\"\"Randomly rotate or flip the image\"\"\"\n",
    "    i = np.random.randint(8)\n",
    "    if i == 0 :\n",
    "        return im\n",
    "    if i == 1 :\n",
    "        return np.rot90(im, axes=(0,1), k=1)\n",
    "    if i == 2 :\n",
    "        return np.rot90(im, axes=(0,1), k=2)\n",
    "    if i == 3 :\n",
    "        return np.rot90(im, axes=(0,1), k=3)\n",
    "    if i == 4:\n",
    "        return np.flip(im, axis=1)\n",
    "    if i == 5:\n",
    "        return np.flip(np.rot90(im, axes=(0,1), k=1))\n",
    "    if i == 6:\n",
    "        return np.flip(np.rot90(im, axes=(0,1), k=2))\n",
    "    if i == 7:\n",
    "        return np.flip(np.rot90(im, axes=(0,1), k=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HackathonDataset(Dataset):\n",
    "    \"\"\"Hackathon Dataset\"\"\"   \n",
    "    \n",
    "    def __init__(self, csv_file, root_dir, use_raw=False, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with paths and labels.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.file = pd.read_csv(csv_file)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.use_raw = use_raw\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.file)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()        \n",
    "        img_file = self.file.iloc[idx][\"image_file_name\"]        \n",
    "        if \"22NCL\" in img_file:\n",
    "            img_dir= \"guyane/guyane/\"\n",
    "        elif \"28PCC\" in img_file:\n",
    "            img_dir = \"saint_louis/saint_louis/\"  \n",
    "        elif \"29SMD\" in img_file:\n",
    "            img_dir = \"dataset_29SMD/dataset_29SMD/\"  \n",
    "        elif \"29TNE\" in img_file:\n",
    "            img_dir = \"dataset_29TNE/dataset_29TNE/\"\n",
    "        else:\n",
    "            raise Exception('There is something wrong with image name')    \n",
    "        image = np.load(self.root_dir + img_dir + img_file + \".npy\")\n",
    "        if self.use_raw:\n",
    "            image_raw = np.load(self.root_dir + img_dir + img_file + \"_RAW.npy\")\n",
    "            image = np.concatenate((image, image_raw), axis=2)\n",
    "        if self.transform:\n",
    "            image = random_transformation(image)  # Add a random permutation of the image\n",
    "        image = np.moveaxis(image, -1, 0)  # Permute dimensions in order to have Cin, H, W instead of H, W, Cin\n",
    "        image = image.astype(np.float32)  # We work with float (float32), not double (float64)\n",
    "        target = self.file.iloc[idx][\"z\"]\n",
    "        target = target.astype(np.float32)  # We work with float (float32), not double (float64)\n",
    "        sample = {'image': image, 'z': target, \"image_file_name\": img_file}\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    \"\"\"Used Network\"\"\"\n",
    "\n",
    "    def __init__(self, device, use_raw = USE_RAW):\n",
    "        self.use_raw = use_raw\n",
    "        super(Net, self).__init__()\n",
    "        self.device = device\n",
    "        self.reset()\n",
    "    \n",
    "    def reset(self):\n",
    "        self.conv1 = nn.Conv2d(8 if self.use_raw else 4, 32, 3, padding=1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.pool1 = nn.MaxPool2d(2)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.pool2 = nn.MaxPool2d(2)\n",
    "#         self.conv3 = nn.Conv2d(32, 256, 3, padding=1)\n",
    "#         self.relu3 = nn.ReLU()\n",
    "#         self.pool3 = nn.MaxPool2d(2)\n",
    "        self.fc1 = nn.Linear(6400, 128)  \n",
    "        self.relu4 = nn.ReLU()\n",
    "#         self.fc2 = nn.Linear(1024, 1024)\n",
    "#         self.relu5 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(128, 1)\n",
    "        self.relu6 = nn.ReLU()\n",
    "        self.to(self.device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Given a tensor X of shape (Batch_size, C_in, H, W), compute the output tensor, of shape (Batch_size, )\"\"\"\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.pool2(x)\n",
    "#         x = self.conv3(x)\n",
    "#         x = self.relu3(x)\n",
    "#         x = self.pool3(x)\n",
    "        x = x.flatten(start_dim=1)  # Flatten the 3 last dimensions, keep the 1 dimension (batch_size)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu4(x)\n",
    "#         x = self.fc2(x)\n",
    "#         x = self.relu5(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.relu6(x)\n",
    "        x = x.flatten()  # The output dimension should be (Batch_size, ) and not (Batch_size, 1) \n",
    "        return x\n",
    "    \n",
    "    def fit(self, train_dataloader, test_dataloader, n_epochs, optimizer=None, criterion=nn.MSELoss()):\n",
    "        \n",
    "        while True:\n",
    "        \n",
    "            if optimizer is None:\n",
    "                optimizer = torch.optim.Adam(self.parameters())\n",
    "            \n",
    "            print_frequency = 2\n",
    "            train_losses = []\n",
    "            test_losses = []\n",
    "            \n",
    "            for epoch in range(n_epochs):\n",
    "                \n",
    "                timer = time.time()\n",
    "                \n",
    "                # Print Epoch\n",
    "                print(f\"Epoch {epoch + 1}/{n_epochs}\")\n",
    "                \n",
    "                # Training loop\n",
    "                for it, batch in enumerate(train_dataloader):\n",
    "                            \n",
    "                    # Reset gradients\n",
    "                    optimizer.zero_grad()\n",
    "                    \n",
    "                    # Forward propagation through the network\n",
    "                    out = self(batch[\"image\"].to(DEVICE))\n",
    "\n",
    "                    # Calculate the loss\n",
    "                    loss = torch.sqrt(criterion(out, batch[\"z\"].to(DEVICE)))  # We take square root because RMSE is the competition's metric\n",
    "\n",
    "                    # Track batch loss\n",
    "                    train_losses.append(loss.item())\n",
    "\n",
    "                    # Backpropagation\n",
    "                    loss.backward()\n",
    "\n",
    "                    # Update the parameters\n",
    "                    optimizer.step()\n",
    "\n",
    "                    #=====Printing part======\n",
    "                    if (it+1)%(len(train_dataloader) // print_frequency) == 0:\n",
    "                        print(f\"Number of sample viewed : {it*batch_size}\")\n",
    "                        print(f\"Current training loss : {np.mean(train_losses[-len(train_dataloader)//print_frequency:-1])}\")\n",
    "\n",
    "                        # Validation loop\n",
    "                        for it, batch in enumerate(test_dataloader):\n",
    "\n",
    "                            # Forward propagation through the network\n",
    "                            out = self(batch[\"image\"].to(DEVICE))\n",
    "\n",
    "                            # Calculate the loss\n",
    "                            loss = torch.sqrt(criterion(out, batch[\"z\"].to(DEVICE)))  # We take square root because RMSE is the competition's metric\n",
    "\n",
    "                            # Track batch loss\n",
    "                            test_losses.append(loss.item())\n",
    "            \n",
    "                        print(f\"Current validation loss : {np.mean(test_losses[-int(len(test_dataloader)*0.8):-1])}\")\n",
    "                \n",
    "                print(f\"The epoch took {time.time() - timer: .2f} seconds\")\n",
    "            if np.mean(train_losses) < 15:\n",
    "                break\n",
    "            \n",
    "            self.reset()\n",
    "            \n",
    "                \n",
    "    def predict(self, dataloader):\n",
    "        predictions = []\n",
    "        for it, batch in enumerate(dataloader):\n",
    "            out = list(self(batch[\"image\"].to(DEVICE)).cpu().detach().numpy())\n",
    "            predictions += out\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Ensemble:\n",
    "    \n",
    "    def __init__(self, Model, device, n_estimators):\n",
    "        self.Model = Model\n",
    "        self.instances = [self.Model(device) for i in range(n_estimators)]\n",
    "    \n",
    "    def fit(self, train_dataloader, test_dataloader, n_epochs):\n",
    "        for it, instance in enumerate(self.instances):\n",
    "            print(f\"=== Training instance {it+1}/{len(self.instances)} ===\")\n",
    "            instance.fit(train_dataloader, test_dataloader, n_epochs)\n",
    "    \n",
    "    def predict(self, dataloader):\n",
    "        predictions = [instance.predict(dataloader) for instance in self.instances]\n",
    "        return np.mean(predictions, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 3\n",
    "n_estimators = 5\n",
    "batch_size = 16  # High batch size often happen to not converge... So we use small batches, even if slower\n",
    "pred_batch_size = 128  # There is no problem of convergence for training batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#========================NOTE============================\n",
    "# We often have to reset the model, because it won't converge. I don't know why, but it is useful to know\n",
    "# If the training loss is stuck around 22 and the validation loss is stuck around 10,\n",
    "# reset the model by running this cell again, and relaunch training\n",
    "#========================END Of NOTE=====================\n",
    "\n",
    "dataset = HackathonDataset(DATA_DIR + 'mixed_train.csv', DATA_DIR, USE_RAW)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=os.cpu_count() - 2)\n",
    "val_dataset = HackathonDataset(DATA_DIR + 'mixed_validation.csv', DATA_DIR, USE_RAW)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=pred_batch_size, shuffle=False, num_workers=os.cpu_count() - 2)\n",
    "model = Ensemble(Net, DEVICE, n_estimators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Training instance 1/5 ===\n",
      "Epoch 1/3\n",
      "Number of sample viewed : 28480\n",
      "Current training loss : 7.3379768899317535\n",
      "Current validation loss : 5.502696065452155\n",
      "Number of sample viewed : 56976\n",
      "Current training loss : 6.542136395111513\n",
      "Current validation loss : 4.1068934174034535\n",
      "The epoch took  14.97 seconds\n",
      "Epoch 2/3\n",
      "Number of sample viewed : 28480\n",
      "Current training loss : 6.1871063851238635\n",
      "Current validation loss : 5.513519717013742\n",
      "Number of sample viewed : 56976\n",
      "Current training loss : 5.832306250829375\n",
      "Current validation loss : 3.979257293573515\n",
      "The epoch took  16.31 seconds\n",
      "Epoch 3/3\n",
      "Number of sample viewed : 28480\n",
      "Current training loss : 5.675735554467426\n",
      "Current validation loss : 5.1043099202509\n",
      "Number of sample viewed : 56976\n",
      "Current training loss : 5.556661059548346\n",
      "Current validation loss : 3.63818284094803\n",
      "The epoch took  16.81 seconds\n",
      "=== Training instance 2/5 ===\n",
      "Epoch 1/3\n",
      "Number of sample viewed : 28480\n",
      "Current training loss : 7.384340753046314\n",
      "Current validation loss : 5.8337614020024695\n",
      "Number of sample viewed : 56976\n",
      "Current training loss : 6.4976342515999015\n",
      "Current validation loss : 5.153890091603197\n",
      "The epoch took  16.43 seconds\n",
      "Epoch 2/3\n",
      "Number of sample viewed : 28480\n",
      "Current training loss : 6.124625499596757\n",
      "Current validation loss : 5.239346981048584\n",
      "Number of sample viewed : 56976\n",
      "Current training loss : 5.85634290348278\n",
      "Current validation loss : 4.744019488650044\n",
      "The epoch took  15.95 seconds\n",
      "Epoch 3/3\n",
      "Number of sample viewed : 28480\n",
      "Current training loss : 5.72955807427342\n",
      "Current validation loss : 4.271833248025789\n",
      "Number of sample viewed : 56976\n",
      "Current training loss : 5.583805021141353\n",
      "Current validation loss : 5.345073723417568\n",
      "The epoch took  16.42 seconds\n",
      "=== Training instance 3/5 ===\n",
      "Epoch 1/3\n",
      "Number of sample viewed : 28480\n",
      "Current training loss : 7.208343794908417\n",
      "Current validation loss : 6.163429585028821\n",
      "Number of sample viewed : 56976\n",
      "Current training loss : 6.216158275389939\n",
      "Current validation loss : 5.061375650833911\n",
      "The epoch took  15.70 seconds\n",
      "Epoch 2/3\n",
      "Number of sample viewed : 28480\n",
      "Current training loss : 5.950097863191969\n",
      "Current validation loss : 6.005045661776085\n",
      "Number of sample viewed : 56976\n",
      "Current training loss : 5.777638082825736\n",
      "Current validation loss : 5.289793945672944\n",
      "The epoch took  15.64 seconds\n",
      "Epoch 3/3\n",
      "Number of sample viewed : 28480\n",
      "Current training loss : 5.641174165988236\n",
      "Current validation loss : 4.814695729045417\n",
      "Number of sample viewed : 56976\n",
      "Current training loss : 5.56902357781871\n",
      "Current validation loss : 4.3359167791727025\n",
      "The epoch took  15.84 seconds\n",
      "=== Training instance 4/5 ===\n",
      "Epoch 1/3\n",
      "Number of sample viewed : 28480\n",
      "Current training loss : 9.949224503388566\n",
      "Current validation loss : 5.170161365524051\n",
      "Number of sample viewed : 56976\n",
      "Current training loss : 6.266094309292482\n",
      "Current validation loss : 4.311348656969746\n",
      "The epoch took  16.21 seconds\n",
      "Epoch 2/3\n",
      "Number of sample viewed : 28480\n",
      "Current training loss : 6.064190077647734\n",
      "Current validation loss : 3.97310988640222\n",
      "Number of sample viewed : 56976\n",
      "Current training loss : 5.804688573553321\n",
      "Current validation loss : 4.818825203602708\n",
      "The epoch took  16.32 seconds\n",
      "Epoch 3/3\n",
      "Number of sample viewed : 28480\n",
      "Current training loss : 5.647891985834314\n",
      "Current validation loss : 4.454829666558213\n",
      "Number of sample viewed : 56976\n",
      "Current training loss : 5.556554064255082\n",
      "Current validation loss : 3.490200883760227\n",
      "The epoch took  15.73 seconds\n",
      "=== Training instance 5/5 ===\n",
      "Epoch 1/3\n",
      "Number of sample viewed : 28480\n",
      "Current training loss : 7.326112994317258\n",
      "Current validation loss : 6.3414125273546835\n",
      "Number of sample viewed : 56976\n",
      "Current training loss : 6.508447308888596\n",
      "Current validation loss : 4.125614430960708\n",
      "The epoch took  15.86 seconds\n",
      "Epoch 2/3\n",
      "Number of sample viewed : 28480\n",
      "Current training loss : 6.024412483847543\n",
      "Current validation loss : 4.079641791779225\n",
      "Number of sample viewed : 56976\n",
      "Current training loss : 5.798135538449448\n",
      "Current validation loss : 4.170066302216898\n",
      "The epoch took  15.91 seconds\n",
      "Epoch 3/3\n",
      "Number of sample viewed : 28480\n",
      "Current training loss : 5.617735253157241\n",
      "Current validation loss : 3.917284567525068\n",
      "Number of sample viewed : 56976\n",
      "Current training loss : 5.512049595693524\n",
      "Current validation loss : 4.942213166417099\n",
      "The epoch took  15.90 seconds\n"
     ]
    }
   ],
   "source": [
    "model.fit(dataloader, val_dataloader, n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smoothed_curves(tab, gamma=0.99):\n",
    "    ret = [tab[min(len(tab)-1, 2000)]]\n",
    "    for val in tab:\n",
    "        ret.append(ret[-1]*gamma + val * (1-gamma))\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.plot(smoothed_curves(train_losses))\n",
    "#plt.show()\n",
    "#plt.plot(smoothed_curves(val_losses))\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation on Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = HackathonDataset(DATA_DIR + 'mixed_test.csv', DATA_DIR, USE_RAW)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=pred_batch_size, shuffle=False, num_workers=os.cpu_count() - 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_file_names = []\n",
    "for val in test_dataloader:\n",
    "    image_file_names += val['image_file_name']\n",
    "\n",
    "predictions = model.predict(test_dataloader)\n",
    "kaggle_df = pd.DataFrame({'image_id': image_file_names,\n",
    "                          'predicted_z': predictions})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "kaggle_df.to_csv('predictions/prediction-' + datetime.now().strftime(\"%d-%m-%y:%H-%M\") + '.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
